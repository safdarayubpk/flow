# Implementation Plan: Dapr Microservices & Pub/Sub

**Branch**: `007-dapr-microservices` | **Date**: 2026-02-15 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/007-dapr-microservices/spec.md`

## Summary

Replace the direct aiokafka event system with Dapr pub/sub (backed by the same Kafka broker) and convert in-process Kafka consumers to Dapr HTTP subscription handlers. Add Dapr service invocation as a second building block. The FastAPI backend runs with a Dapr sidecar in self-hosted mode via `dapr run`. All existing functionality (REST API, UI, AI chat, user isolation) remains unchanged.

## Technical Context

**Language/Version**: Python 3.13+
**Primary Dependencies**: FastAPI, SQLModel, dapr (Python SDK), dapr-ext-fastapi, existing Neon PostgreSQL
**Storage**: Neon PostgreSQL (unchanged), Kafka via docker-compose (unchanged)
**Testing**: pytest, manual smoke tests via curl + Kafka UI
**Target Platform**: Linux local development (Dapr self-hosted mode)
**Project Type**: Web application (monorepo: backend + frontend)
**Performance Goals**: Fire-and-forget event publishing; no added latency to REST API responses
**Constraints**: Single FastAPI process + Dapr sidecar; no Kubernetes; no new databases
**Scale/Scope**: Local development; same user base as Phase V.2

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

| Principle | Status | Evidence |
|-----------|--------|----------|
| I. Spec-Driven Development | PASS | spec.md complete with 5 user stories, 11 FRs, 8 SCs, 3 clarifications |
| II. AI-Only Implementation | PASS | All code will be generated by Claude Code |
| III. Iterative Evolution | PASS | Builds on Phase V.2 Kafka; maintains backward compatibility |
| IV. Reusability and Modularity | PASS | Dapr publisher as reusable module; skills created (dapr-pubsub-pattern, dapr-invocation-pattern) |
| V. Security and Isolation | PASS | user_id propagated in Dapr metadata; validated in all subscription handlers |
| VI. Cloud-Native Readiness | PASS | Dapr is cloud-native by design; self-hosted now, Kubernetes in Phase V.4 |

**Gate Result**: ALL PASS — proceed to implementation planning.

## Project Structure

### Documentation (this feature)

```text
specs/007-dapr-microservices/
├── spec.md                  # Feature specification
├── plan.md                  # This file
├── research.md              # Phase 0 research output
├── data-model.md            # Data model (no schema changes)
├── quickstart.md            # Developer quickstart guide
├── contracts/
│   ├── dapr-subscriptions.md  # Subscription endpoint contracts
│   └── dapr-components.md     # Dapr component YAML contracts
└── tasks.md                 # Phase 2 output (via /sp.tasks)
```

### Source Code (repository root)

```text
backend/
├── components/                          # NEW: Dapr component configs
│   └── kafka-pubsub.yaml               # Dapr pub/sub → Kafka bridge
├── backend/
│   └── src/
│       ├── main.py                      # MODIFY: Add DaprApp, remove consumer startup
│       ├── core/
│       │   └── config.py                # MODIFY: Add Dapr settings, remove Kafka consumer settings
│       ├── services/
│       │   ├── dapr/                    # NEW: Dapr integration module
│       │   │   ├── __init__.py          # Module init with publisher + subscription setup
│       │   │   ├── publisher.py         # Dapr pub/sub publisher (replaces kafka/producer.py)
│       │   │   ├── subscriptions.py     # Dapr subscription handlers (replaces kafka/consumer.py + handlers.py)
│       │   │   └── service_invocation.py  # Dapr service invocation endpoint
│       │   ├── task_service.py          # MODIFY: Replace fire_event() → dapr publisher
│       │   ├── recurring_service.py     # MODIFY: Replace fire_event() → dapr publisher
│       │   └── kafka/                   # REMOVE: Entire directory after migration
│       │       ├── __init__.py
│       │       ├── producer.py
│       │       ├── consumer.py
│       │       ├── handlers.py
│       │       └── events.py            # KEEP: EventTypes constants (move to dapr/)
│       └── api/
│           └── v1/
│               └── tasks.py             # NO CHANGE: REST API untouched
└── requirements.txt                     # MODIFY: Add dapr, dapr-ext-fastapi; remove aiokafka
```

**Structure Decision**: Existing monorepo web application structure preserved. New `backend/components/` directory for Dapr YAML configs. New `backend/backend/src/services/dapr/` module replaces `services/kafka/`. No frontend changes.

## Implementation Phases

### Phase A: Dapr Infrastructure Setup

**Goal**: Install Dapr, create component configs, verify sidecar runs with existing Kafka.

**Tasks**:

1. **T001: Install Dapr CLI and initialize runtime**
   - Install Dapr CLI via official script
   - Run `dapr init` to set up self-hosted runtime
   - Verify with `dapr --version` and `docker ps`
   - *Verification*: `dapr --version` outputs version; placement container running

2. **T002: Create Dapr kafka-pubsub component YAML**
   - Create `backend/components/kafka-pubsub.yaml`
   - Configure `pubsub.kafka` type pointing to `localhost:9092`
   - Set `consumerGroup: dapr-todo-backend`, `authRequired: false`
   - *Verification*: YAML is valid; file exists at correct path
   - *Ref*: contracts/dapr-components.md

3. **T003: Add Dapr Python dependencies**
   - Add `dapr` and `dapr-ext-fastapi` to `backend/requirements.txt`
   - Install via `pip install dapr dapr-ext-fastapi`
   - *Verification*: `python -c "from dapr.clients import DaprClient; from dapr.ext.fastapi import DaprApp; print('OK')"`

4. **T004: Verify Dapr sidecar starts with existing app**
   - Start Kafka: `docker-compose -f docker-compose.kafka.yml up -d`
   - Run: `dapr run --app-id todo-backend --app-port 8000 --resources-path ./backend/components -- uvicorn backend.backend.src.main:app --host 0.0.0.0 --port 8000`
   - Verify sidecar health: `curl http://localhost:3500/v1.0/healthz`
   - Verify app health: `curl http://localhost:8000/docs`
   - *Verification*: Both endpoints respond OK; existing REST API works unchanged

### Phase B: Dapr Pub/Sub Publisher (Replace aiokafka Producer)

**Goal**: Create Dapr publisher module that replaces `fire_event()` from `kafka/producer.py`.

**Skills used**: dapr-pubsub-pattern, user-isolation-enforcer

**Tasks**:

5. **T005: Create Dapr publisher module**
   - Create `backend/backend/src/services/dapr/__init__.py`
   - Create `backend/backend/src/services/dapr/publisher.py`
   - Implement `publish_event(topic, event_data, user_id)` using `DaprClient().publish_event()`
   - Fire-and-forget: catch all exceptions, log WARNING, return False
   - Structured INFO logging: event_type, user_id, status on every publish
   - Include `user_id` in `publish_metadata` for Dapr metadata propagation
   - *Ref*: dapr-pubsub-pattern skill, FR-001, FR-005, FR-010, FR-011

6. **T006: Create fire_event() compatibility wrapper**
   - In `publisher.py`, implement `fire_event(event_type, user_id, payload, topic)` matching the existing signature from `kafka/producer.py`
   - Internally calls `publish_event()` with Dapr
   - Builds same event envelope: `{"event_type": ..., "timestamp": ..., "user_id": ..., "payload": ...}`
   - *Ref*: FR-002 (identical event envelope format)

7. **T007: Move EventTypes constants to Dapr module**
   - Copy `EventTypes` class from `kafka/events.py` to `backend/backend/src/services/dapr/events.py`
   - Keep same event type string values
   - *Verification*: All 7 event types preserved

8. **T008: Update task_service.py to use Dapr publisher**
   - Replace `from services.kafka.producer import fire_event` with `from services.dapr.publisher import fire_event`
   - All 7 `fire_event()` calls in task_service.py remain unchanged (compatibility wrapper)
   - *Verification*: task_service.py imports from dapr module; no other code changes needed
   - *Ref*: FR-001, FR-008

9. **T009: Update recurring_service.py to use Dapr publisher**
   - Replace `from services.kafka.producer import fire_event` with `from services.dapr.publisher import fire_event`
   - `fire_event()` call for `reminder-triggered` remains unchanged
   - *Verification*: recurring_service.py imports from dapr module
   - *Ref*: FR-001, FR-007

10. **T010: Verify event publishing via Dapr**
    - Start app with `dapr run`
    - Create a task via REST API
    - Check Kafka UI (port 8080) for the event
    - Verify event envelope matches Phase V.2 format
    - *Verification*: SC-001 partial — event visible in Kafka UI with correct format

### Phase C: Dapr Subscriptions (Replace Kafka Consumers)

**Goal**: Replace in-process KafkaEventConsumer loops with Dapr HTTP subscription handlers.

**Skills used**: dapr-pubsub-pattern, user-isolation-enforcer, fastapi-jwt-user-context

**Tasks**:

11. **T011: Create Dapr subscription handlers module**
    - Create `backend/backend/src/services/dapr/subscriptions.py`
    - Implement notification handler: processes `task-created` and `reminder-triggered` events
    - Implement recurring handler: processes `recurring-task-created` events
    - Both handlers extract event data from CloudEvents `data` field
    - Both handlers validate `user_id` presence; skip with WARNING log if missing
    - Recurring handler preserves idempotency check (query DB for existing instance before creating)
    - *Ref*: FR-003, FR-004, FR-005, FR-011, existing `kafka/handlers.py` logic

12. **T012: Register DaprApp and subscriptions in main.py**
    - Import `DaprApp` from `dapr.ext.fastapi`
    - Wrap FastAPI app: `dapr_app = DaprApp(app)`
    - Register notification subscription: `@dapr_app.subscribe(pubsub='kafka-pubsub', topic='todo-events')`
    - Register recurring subscription: `@dapr_app.subscribe(pubsub='kafka-pubsub', topic='todo-events')`
    - Route notification events to `/api/dapr/notifications`
    - Route recurring events to `/api/dapr/recurring`
    - *Ref*: FR-003, FR-004, FR-009, contracts/dapr-subscriptions.md

13. **T013: Update main.py lifespan — remove Kafka consumers**
    - Remove `notification_consumer` background task startup
    - Remove `recurring_consumer` background task startup
    - Remove consumer shutdown logic
    - Keep `get_producer()` removal for later (T016)
    - Keep recurring scheduler background task (FR-007)
    - *Ref*: FR-003, FR-004, FR-007

14. **T014: Verify subscriptions fire on events**
    - Start app with `dapr run`
    - Create a task → check logs for notification handler invocation
    - Create a task with recurrence_rule → check logs for recurring handler invocation
    - Verify user_id is logged in structured format
    - *Verification*: SC-002 partial — subscription handlers fire correctly

### Phase D: Service Invocation (Second Dapr Building Block)

**Goal**: Add Dapr service invocation endpoint for health/status.

**Skills used**: dapr-invocation-pattern

**Tasks**:

15. **T015: Create service invocation endpoint**
    - Create `backend/backend/src/services/dapr/service_invocation.py`
    - Implement `GET /api/dapr/health` endpoint returning:
      - `status: "healthy"`, `app_id: "todo-backend"`, `dapr_enabled: true`
      - `pubsub_component: "kafka-pubsub"`, `subscriptions_active: 2`
      - Event publish/receive counters (in-memory atomic counters)
    - Register router in main.py
    - *Ref*: FR-006, SC-004, contracts/dapr-subscriptions.md

16. **T016: Verify service invocation**
    - Start app with `dapr run`
    - Test via Dapr CLI: `dapr invoke --app-id todo-backend --method api/dapr/health --verb GET`
    - Test via Dapr HTTP: `curl http://localhost:3500/v1.0/invoke/todo-backend/method/api/dapr/health`
    - *Verification*: SC-004 — both pub/sub and service invocation working

### Phase E: Cleanup and Configuration

**Goal**: Remove old aiokafka code, update config, finalize dependencies.

**Tasks**:

17. **T017: Update config.py — Dapr settings**
    - Add `DAPR_PUBSUB_NAME` setting (default: "kafka-pubsub")
    - Add `DAPR_APP_ID` setting (default: "todo-backend")
    - Add `DAPR_ENABLED` setting (default: True, for graceful degradation)
    - Keep `KAFKA_BOOTSTRAP_SERVERS` for reference in Dapr component YAML
    - Remove `KAFKA_NOTIFICATION_GROUP_ID` and `KAFKA_RECURRING_GROUP_ID` (no longer needed)
    - *Ref*: FR-009, FR-010

18. **T018: Remove old aiokafka code**
    - Delete `backend/backend/src/services/kafka/producer.py`
    - Delete `backend/backend/src/services/kafka/consumer.py`
    - Delete `backend/backend/src/services/kafka/handlers.py`
    - Delete `backend/backend/src/services/kafka/__init__.py`
    - Delete `backend/backend/src/services/kafka/events.py` (already moved to dapr/events.py in T007)
    - Remove `aiokafka` from requirements.txt
    - Remove Kafka producer startup/shutdown from main.py lifespan (if not already done)
    - *Ref*: FR-001 (clean replacement confirmed in clarification)

19. **T019: Update requirements.txt**
    - Add: `dapr>=1.14.0`, `dapr-ext-fastapi>=1.14.0`
    - Remove: `aiokafka>=0.10.0`
    - Keep all other dependencies unchanged
    - *Verification*: `pip install -r requirements.txt` succeeds

### Phase F: Testing and Verification

**Goal**: Validate all success criteria.

**Tasks**:

20. **T020: Smoke test — all 7 event types (SC-001)**
    - Execute each CRUD operation once via REST API
    - Inspect Kafka UI for all 7 event types
    - Verify event envelope format matches Phase V.2
    - *Verification*: SC-001 complete

21. **T021: Smoke test — notification subscriptions (SC-002)**
    - Execute 10 task operations (create, update, complete, delete mix)
    - Check application logs for notification handler invocations
    - Verify zero missed deliveries
    - *Verification*: SC-002 complete

22. **T022: Smoke test — recurring subscriptions (SC-003)**
    - Create task with recurrence_rule
    - Verify next instance created by subscription handler
    - Send duplicate event (re-publish same event)
    - Verify idempotency — no duplicate instance
    - *Verification*: SC-003 complete

23. **T023: Smoke test — user isolation (SC-008)**
    - Send test event without user_id in metadata
    - Verify handler skips event and logs WARNING
    - Create task as user A, verify event contains user A's user_id
    - *Verification*: SC-008 complete

24. **T024: Regression test — existing API (SC-007)**
    - Run existing API endpoint tests (if available)
    - Manual regression: test task CRUD, AI chat, auth endpoints
    - Verify all respond identically to pre-Dapr behavior
    - *Verification*: SC-007 complete

25. **T025: Smoke test — local dev workflow (SC-005)**
    - From clean state: `docker-compose up -d` + `dapr run`
    - Time to fully operational state
    - Verify all endpoints, subscriptions, and service invocation work
    - *Verification*: SC-005 complete

26. **T026: Event count verification (SC-006)**
    - Execute 20 task operations
    - Compare event count in Kafka UI vs. DB write count
    - Verify exact match (zero orphan or duplicate events)
    - *Verification*: SC-006 complete

## Task Dependency Graph

```
T001 (Dapr install)
  → T002 (Component YAML)
  → T003 (Python deps)
    → T004 (Verify sidecar)
      → T005 (Publisher module)
        → T006 (Compatibility wrapper)
        → T007 (EventTypes)
          → T008 (Update task_service)
          → T009 (Update recurring_service)
            → T010 (Verify publishing)
              → T011 (Subscription handlers)
                → T012 (Register DaprApp)
                → T013 (Remove consumers from lifespan)
                  → T014 (Verify subscriptions)
                    → T015 (Service invocation)
                      → T016 (Verify invocation)
                        → T017 (Config update)
                        → T018 (Remove aiokafka)
                        → T019 (Update requirements)
                          → T020-T026 (Testing)
```

## Complexity Tracking

No constitution violations. All changes follow existing patterns:

| Decision | Rationale |
|----------|-----------|
| New `services/dapr/` module | Replaces `services/kafka/` 1:1; same module boundary pattern |
| DaprApp wrapper in main.py | Minimal change; wraps existing app without altering routes |
| `fire_event()` compatibility wrapper | Minimizes diff in task_service.py and recurring_service.py |
| In-memory event counters | Lightweight; no persistence needed for local dev health endpoint |

## Risks Addressed in Plan

| Risk | Mitigation in Plan |
|------|-------------------|
| Dapr sidecar latency | T005: fire-and-forget pattern preserves API speed |
| Subscription delivery semantics | T011: idempotency checks preserved from existing handlers |
| Local dev complexity | quickstart.md + T004 verification step |
| Event format compatibility | T006: compatibility wrapper produces identical envelope |